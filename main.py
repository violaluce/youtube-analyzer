import os
import json
import time
import requests
import re
import pandas as pd
import gspread
import urllib3
import isodate
from bs4 import BeautifulSoup
from googleapiclient.discovery import build
from oauth2client.service_account import ServiceAccountCredentials
from datetime import datetime

# --- åˆæœŸè¨­å®š ---
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# GitHub Secretsã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€
YOUTUBE_API_KEY = os.environ.get('YOUTUBE_API_KEY')
GOOGLE_JSON_DATA = os.environ.get('GOOGLE_JSON_DATA')

def get_yutura_list():
    """ãƒ¦ãƒ¼ãƒãƒ¥ãƒ©ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‹ã‚‰ãƒãƒ£ãƒ³ãƒãƒ«URLã‚’å–å¾—"""
    # 2026å¹´2æœˆã®å†ç”Ÿå›æ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°
    url = "https://yutura.net/ranking/mon/?mode=view&date=202602&p=1"
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
    
    try:
        res = requests.get(url, headers=headers, verify=False, timeout=15)
        res.encoding = res.apparent_encoding
        soup = BeautifulSoup(res.text, 'html.parser')
        
        channels = []
        for a in soup.find_all('a'):
            href = a.get('href', '')
            # ãƒãƒ£ãƒ³ãƒãƒ«å€‹åˆ¥ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
            if '/channel/' in href and a.text.strip() and 'ãƒãƒ£ãƒ³ãƒãƒ«ã®è©³ç´°' not in a.text:
                channels.append({"name": a.text.strip(), "url": "https://yutura.net" + href})
        
        # é‡è¤‡å‰Šé™¤
        unique_channels = []
        seen = set()
        for c in channels:
            if c['url'] not in seen:
                unique_channels.append(c)
                seen.add(c['url'])
        return unique_channels
    except Exception as e:
        print(f"ãƒ¦ãƒ¼ãƒãƒ¥ãƒ©å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def get_yt_id(yutura_url):
    """ãƒ¦ãƒ¼ãƒãƒ¥ãƒ©è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰YouTube ID (UC...) ã‚’æŠ½å‡º"""
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        res = requests.get(yutura_url, headers=headers, verify=False, timeout=10)
        match = re.search(r'youtube\.com/channel/(UC[\w-]+)', res.text)
        return match.group(1) if match else None
    except:
        return None

def main():
    print("ğŸš€ 1. ãƒ¦ãƒ¼ãƒãƒ¥ãƒ©ã‹ã‚‰ãƒªã‚¹ãƒˆã‚’å–å¾—ä¸­...")
    raw_list = get_yutura_list()
    
    if not raw_list:
        print("ãƒªã‚¹ãƒˆãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚çµ‚äº†ã—ã¾ã™ã€‚")
        return

    print(f"   {len(raw_list)}ä»¶ã®å€™è£œã‚’ç™ºè¦‹ã€‚YouTube IDã‚’ç‰¹å®šä¸­...")
    
    data_for_api = []
    # APIç¯€ç´„ã®ãŸã‚ã€ã¾ãšã¯IDã‚’ç‰¹å®šï¼ˆä¸Šä½20ä»¶ç¨‹åº¦ã§ãƒ†ã‚¹ãƒˆï¼‰
    for item in raw_list[:20]:
        uid = get_yt_id(item['url'])
        if uid:
            data_for_api.append(uid)
        time.sleep(1) # ã‚µã‚¤ãƒˆã¸ã®è² è·è»½æ¸›

    print(f"ğŸ“Š 2. YouTube APIã§è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­ ({len(data_for_api)}ä»¶)...")
    youtube = build('youtube', 'v3', developerKey=YOUTUBE_API_KEY)
    
    # ã¾ã¨ã‚ã¦å–å¾—ï¼ˆæœ€å¤§50ä»¶ã¾ã§1é€šä¿¡ï¼‰
    ch_res = youtube.channels().list(
        id=','.join(data_for_api),
        part='snippet,statistics'
    ).execute()

    final_data = []
    for item in ch_res['items']:
        final_data.append({
            "æ—¥ä»˜": datetime.now().strftime('%Y-%m-%d'),
            "åå‰": item['snippet']['title'],
            "ç™»éŒ²è€…æ•°": int(item['statistics']['subscriberCount']),
            "ç·å†ç”Ÿæ•°": int(item['statistics']['viewCount']),
            "å‹•ç”»æ•°": int(item['statistics']['videoCount']),
            "URL": f"https://www.youtube.com/channel/{item['id']}"
        })
    
    df_new = pd.DataFrame(final_data)

    print("ğŸ“ 3. ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸æ›¸ãè¾¼ã¿ä¸­...")
    scope = ['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive']
    creds_dict = json.loads(GOOGLE_JSON_DATA)
    creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)
    client = gspread.authorize(creds)
    
    # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆåã«åˆã‚ã›ã¦å¤‰æ›´ã—ã¦ãã ã•ã„
    sheet = client.open("YouTubeåˆ†æã‚·ãƒ¼ãƒˆ").sheet1

    # B1ã‚»ãƒ«ã«å…¥åŠ›ã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å–å¾—ã™ã‚‹
    search_keyword = sheet.acell('B1').value
    print(f"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‹ã‚‰å–å¾—ã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {search_keyword}")

    # ã“ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ä½¿ã£ã¦æ¤œç´¢ãƒ­ã‚¸ãƒƒã‚¯ã‚’å›ã™
    # url = f"https://yutura.net/ranking/?q={search_keyword}" ...ã®ã‚ˆã†ãªå½¢
    
    # ã‚¯ãƒªã‚¢ã—ã¦ä¸Šæ›¸ãï¼ˆå·®åˆ†è¨ˆç®—ã¯ã‚·ãƒ¼ãƒˆå´ã®é–¢æ•°ã§ã‚‚å¯¾å¿œå¯èƒ½ï¼‰
    sheet.clear()
    header = [df_new.columns.values.tolist()]
    values = df_new.values.tolist()
    sheet.update('A1', header + values)
    
    print("âœ… å…¨å·¥ç¨‹ãŒå®Œäº†ã—ã¾ã—ãŸï¼")

if __name__ == "__main__":
    main()
